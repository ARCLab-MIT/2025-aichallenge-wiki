# Baseline solutions
Two baseline approaches are provided together with the devkit. These baselines will help facilitate the assessment of more advanced AI solutions and help familiarize the participants with the dataset, methodologies, and evaluation pipeline. Participants who are new to the field can leverage the baseline implementation as a starting point and build upon the baseline implementation by iterating on the existing model and experimenting with modifications and incorporating newer AI techniques. 

## Heuristic-based approach
The heuristic-based approach detects changes in the satellite's longitude and inclination waveform by analyzing the variation in their standard deviation. The localized standard deviation is calculated for each data point over the last orbital period---around 24 hours for GEO satellites.  Subsequently, the algorithmic PoL characterization model examines each data point over the six-month period and identifies instances where the standard deviation is greater than a threshold associated with the most common varieties of station-keeping: the maximum value typically observed within station-keeping behavioral modes. The first occurrence of such an event indicates a potential initiate drift (ID) node, suggesting that the satellite has transitioned from a station-keeping behavioral mode to a drifting behavioral mode. During the period of elevated standard deviation, an adjust drift (AD) node is assigned if the variation in consecutive standard deviation values is greater than 10%. This would indicate that the satellite's drift rate or direction has changed during the longitudinal shift maneuver.

When the standard deviation falls back below the stationary mode threshold, this would indicate that the satellites might have transitioned back to a station-keeping behavioral mode. During the low standard deviation period, the oscillation frequency and amplitude of the waveform are analyzed to determine the propulsion type utilized in the station-keeping behavioral modes. 

## Machine learning-based approach
A random forest classifier model was developed as the machine learning baseline approach for this challenge. The model takes in the multi-dimensional satellite state data and predicts the PoL node labels at each time step. To enable classic tabular machine learning, the model has to be trained to predict labels at each time step (row in the table), but its performance must be evaluated based on change point detection accuracy compared to the ground truth. Additionally, since there can be more than one node at the same time step (one for EW and one for NS), separate models have to be trained for each direction. 

To meet these requirements, the ground truth data is first separated into EW and NS types. The “node” and “type”  columns are concatenated to create a single label for both directions EW and NS, such as 'SS-CK'. These labels are forward filled so each row has a label. Additionally, the input data is expanded by creating lagged features from the raw feature columns (position, velocity, orbital elements, etc). Lagged features capture historical context by using the prior n time steps of each feature. The final data table with labels is then split into training and validation sets by satellite ObjectID to avoid data leakage.

The model predictions are post-processed to convert the per time step predictions into a format matching the ground truth. To do this, the predicted EW and NS labels are first split back into the original “node” and “type” columns. Then, they are filtered to only keep rows where the label changes compared to the prior time step. This process converts both the ground truth and predictions into a consistent format with labels provided only at change points.
